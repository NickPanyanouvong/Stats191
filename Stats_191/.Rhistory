# Find the optimal lambda
bc <- boxcox(boxcox_overall_model)
lambda <- bc[[1]][which.max(bc[[2]])] # This is the lambda we'll use for Box-Cox
# Transform the scores column
boxcox_all_games$`game score` <- ((boxcox_all_games$`game score`)^(lambda) - 1)/lambda
# Run regression with transformed scores
fin_boxcox_overall_model <- lm(`game score` ~ `percent training sessions attended`
+ `overall fitness score`
+ `# extra strategy sessions attended`
+ `hours of sleep the night before game`
+ `# meals on day prior to game`
+ `university year`
+ `curling` + `gymnastics` + `baseball` + `martial_arts`
+ `frisbee` + `table_tennis` + `basketball`
+ evening + morning + night_owl
+ night_owl * morning + night_owl * evening,
data=boxcox_all_games)
# Fit the model and plot residuals
boxcox_all_games <- subset(boxcox_all_games, select =-c(`fit`, `se.fit`, `residual`))
boxcox_all_games <- subset(cbind(boxcox_all_games,predict(fin_boxcox_overall_model, se.fit = TRUE)) %>%
mutate(residual = `game score` - `fit`), select = -c(`df`,`residual.scale`))
standard_residuals <- rstandard(fin_boxcox_overall_model)
png("../OUTPUTS/boxcox_residuals.png")
# Plot
ggplot(boxcox_all_games, aes(x = fit, y = standard_residuals)) +
geom_point(aes(color = night_owl)) +
ggtitle("Box-Cox Residuals") +
theme(plot.title = element_text(hjust = 0.5, vjust = 0.5))
dev.off()
#############################################################################
# ALL GAMES ANALYSIS
## Errors are approximately normal
ggplot(data.frame(overallmodel$residuals), aes(x=overallmodel$residuals)) +
geom_histogram(binwidth=1, color="grey50", fill="lightblue", alpha=0.8) +
theme_minimal() +
stat_function(fun = function(x){dnorm(x,mean = mean(overallmodel$residuals),
sd = sd(overallmodel$residuals))}*length(overallmodel$residuals)*1)
# Number of meals is initially positively correlated
model <- lm(`game score` ~ `# meals on day prior to game`,
data=all_games)
summary(model)
ggplot(all_games, aes(x=`# meals on day prior to game`,y=`game score`)) +
stat_smooth(method = "lm") +
geom_point() +
theme_minimal()
# But after accounting for training sessions attended, it's negatively correlated (and significantly so)
model <- lm(`game score` ~ `# meals on day prior to game` + `percent training sessions attended`,
data=all_games)
summary(model)
ggplot(all_games, aes(x=`# meals on day prior to game`,y=`percent training sessions attended`)) +
stat_smooth(method = "lm") +
geom_point() +
theme_minimal()
# There's two ways to manage this: we can say that there might be an unknown
# causal relationship present and so it's simply impossible to vary the number
# of meals eaten without affecting the other covariates, which is what the
# beta value represents (think of the midterm). OR we can ignore that and say
# we should have them eat less meals for better coaching - I think the former
# is better.
## Check for correlation between covariates
used_covariates <- c("percent training sessions attended",
"overall fitness score",
"# extra strategy sessions attended",
"hours of sleep the night before game",
"# meals on day prior to game",
"university year",
"curling", "gymnastics", "baseball", "martial_arts",
"frisbee", "table_tennis", "basketball",
"evening", "morning", "night_owl")
correlations <- c()
for(var in used_covariates) {
correlations <- append(correlations, summary(lm(as.formula(paste("`", var, "`~.", sep = "")),data =
all_games[,names(all_games) %in% used_covariates]))$r.squared)
}
correlations_by_covar <- data.frame(used_covariates,correlations)
if(length(correlations_by_covar$correlations[correlations_by_covar$correlations >= 0.8]) > 0) {
print("Some variables are very correlated (inflation factor > 5)!")
}
## Check for non-linearity
ggplot(all_games, aes(x = `percent training sessions attended`, y = residual)) +
geom_point(color = 'black') +
ggtitle("Percent training sessions attended residuals")
ggplot(all_games, aes(x = `overall fitness score`, y = residual)) +
geom_point(color = 'black') +
ggtitle("Overall fitness score residuals")
ggplot(all_games, aes(x = `# extra strategy sessions attended`, y = residual)) +
geom_point(color = 'black') +
ggtitle("# extra strategy sessions attended residuals")
ggplot(all_games, aes(x = `hours of sleep the night before game`, y = residual)) +
geom_point(color = 'black') +
ggtitle("Hours of sleep the night before game residuals")
ggplot(all_games, aes(x = `# meals on day prior to game`, y = residual)) +
geom_point(color = 'black') +
ggtitle("# meals on day prior to game the night before game residuals")
## Models for morning and evening games
# This is useful for checking whether our model is a good fit -
# specifically, whether the relationship between game score and
# the other covariates depends on whether it's a morning or evening game.
# Notably, the most significant difference (by far) between the two is
# for the night_owl indicator variable - which is what we expect to see,
# intuitively. None of the other ones are particularly significant,
# especially with the multiple testing fallacy. There is also a significant
# difference in the intercept - which makes sense too, because the intercept
# represents early birds. So early birds have an 11 point difference (after
# accounting for changes in the other variables) between morning and evening
# games, while night owls only have about a 3 point difference
eveningmodel <- lm(`game score` ~ `percent training sessions attended`
+ `overall fitness score`
+ `# extra strategy sessions attended`
+ `hours of sleep the night before game`
+ `# meals on day prior to game`
+ `university year` + `curling` + `gymnastics` + `baseball` + `martial_arts`
+ `frisbee` + `table_tennis` + `basketball`
+ night_owl,
data=filter(all_games, evening == 1))
summary(eveningmodel)
morningmodel <- lm(`game score` ~ `percent training sessions attended`
+ `overall fitness score`
+ `# extra strategy sessions attended`
+ `hours of sleep the night before game`
+ `# meals on day prior to game`
+ `university year` + `curling` + `gymnastics` + `baseball` + `martial_arts`
+ `frisbee` + `table_tennis` + `basketball`
+ night_owl,
data=filter(all_games, morning == 1))
summary(morningmodel)
# -----------------------------------------------------------------------------
## MORNING AND EVENING RESIDUAL PLOTS
#------------------------------------------------------------------------------
# Let's explore the residual plots of each of these models. Start w/ morning:
morning_residuals <- filter(all_games, morning == 1) %>%
select(`game score`, `early bird/night owl`)
# -----------------------------------------------------------------------------
## MORNING AND EVENING RESIDUAL PLOTS
#------------------------------------------------------------------------------
# Let's explore the residual plots of each of these models. Start w/ morning:
morning_residuals <- filter(all_games, morning == 1) %>%
dplyr::select(`game score`, `early bird/night owl`)
# Make a hypothetical version of the data where every game was played
# in the or in the evening
morningized_games <- all_games
morningized_games$morning <- 1
morningized_games$evening <- 0
eveningized_games <- all_games
eveningized_games$morning <- 0
eveningized_games$evening <- 1
# Set the variables we see as mutable to the average to remove their effects.
# We decided the training and strategy sessions could be controlled
# the coach on real teams, so we shouldn't consider their impact
# on an individual's score.
morningized_games$`percent training sessions attended` <- max(all_games$`percent training sessions attended`)
morningized_games$`# extra strategy sessions attended` <- max(all_games$`# extra strategy sessions attended`)
eveningized_games$`percent training sessions attended` <- max(all_games$`percent training sessions attended`)
eveningized_games$`# extra strategy sessions attended` <- max(all_games$`# extra strategy sessions attended`)
# Then predict how the players would have scored. We can use
# overallmodel for this, because we know that the only night_owl
# has a significant difference between morning and evening games
# and its interaction with that has been accounted for.
# It's better to use overallmodel to avoid bias due to
# non-statistically significant, but still potentially impactful,
# differences between overallmodel and morning/evening model
morningized_games$`game score` <- predict.lm(overallmodel, morningized_games)
eveningized_games$`game score` <- predict.lm(overallmodel, eveningized_games)
# Adjust by the residual, in case there is some confounding variable
# biasing them. (The residual in these dataframes is still the
# residual from all_games, which is useful here.)
morningized_games$`game score` <- morningized_games$`game score` + morningized_games$residual
eveningized_games$`game score` <- eveningized_games$`game score` + eveningized_games$residual
# Then, average these by player
morning_players <- morningized_games %>% group_by(`student label`) %>% summarize(
mean = mean(`game score`),
sd = sd(`game score`)
)
top_morning <- top_n(morning_players,20,mean)
top_morning <- top_morning[order(top_morning$mean,decreasing = TRUE),]
evening_players <- eveningized_games %>% group_by(`student label`) %>% summarize(
mean = mean(`game score`),
sd = sd(`game score`)
)
top_evening <- top_n(evening_players,20,mean)
top_evening <- top_evening[order(top_evening$mean,decreasing = TRUE),]
best_morning <- c()
best_evening <- c()
for(i in 1:20) {
if(!(top_morning$`student label`[i] %in% best_evening)) {
if(top_morning$`student label`[i] ==
top_evening$`student label`[i]) {
if(top_morning$mean[i] - mean(top_morning$mean) >
top_evening$mean[i] - mean(top_evening$mean)) {
best_morning <- append(best_morning, top_morning$`student label`[i])
} else {
best_evening <- append(best_evening, top_evening$`student label`[i])
}
} else {
best_morning <- append(best_morning, top_morning$`student label`[i])
best_evening <- append(best_evening, top_evening$`student label`[i])
}
}
}
# Keep only the top 10 from each. Those are our teams!
best_morning <- best_morning[1:10]
write.csv(data.frame(best_morning),"../PROCESSED_DATA/west_coast_team.csv")
best_evening <- best_evening[1:10]
write.csv(data.frame(best_evening),"../PROCESSED_DATA/east_coast_team.csv")
#--------------------------------------------------------------------------
## Find our regular season team
#--------------------------------------------------------------------------
# Create another set for all noon games
noonized_games <- all_games
noonized_games$morning <- 0
noonized_games$evening <- 0
# Remove the players used for east coast/west coast cups
noonized_games <- filter(noonized_games,!(`student label` %in% best_morning))
noonized_games <- filter(noonized_games,!(`student label` %in% best_evening))
morningized_games <- filter(noonized_games,!(`student label` %in% best_morning))
morningized_games <- filter(noonized_games,!(`student label` %in% best_evening))
eveningized_games <- filter(noonized_games,!(`student label` %in% best_morning))
eveningized_games <- filter(noonized_games,!(`student label` %in% best_evening))
# Again, we want to remove the effect of the training and strategy sessions.
# However, because we care about absolute scores, and not relative scores,
# we can't just assign them all to the mean. Instead, it makes more sense
# to assign them to the max: we want our players to be the best, so
# we will send them to as many training and strategy sessions as possible.
noonized_games$`percent training sessions attended` <- max(all_games$`percent training sessions attended`)
noonized_games$`# extra strategy sessions attended` <- max(all_games$`# extra strategy sessions attended`)
noonized_games$`game score` <- predict.lm(overallmodel, noonized_games)
noonized_games$`game score` <- noonized_games$`game score` + noonized_games$residual
morningized_games$`percent training sessions attended` <- max(all_games$`percent training sessions attended`)
morningized_games$`# extra strategy sessions attended` <- max(all_games$`# extra strategy sessions attended`)
morningized_games$`game score` <- predict.lm(overallmodel, morningized_games)
morningized_games$`game score` <- morningized_games$`game score` + morningized_games$residual
eveningized_games$`percent training sessions attended` <- max(all_games$`percent training sessions attended`)
eveningized_games$`# extra strategy sessions attended` <- max(all_games$`# extra strategy sessions attended`)
eveningized_games$`game score` <- predict.lm(overallmodel, eveningized_games)
eveningized_games$`game score` <- eveningized_games$`game score` + eveningized_games$residual
adjusted_games <- rbind(noonized_games,morningized_games,eveningized_games)
# Summarize the average of the three by player
all_players <- adjusted_games %>% group_by(`student label`) %>% summarize(
mean = mean(`game score`),
sd = sd(`game score`)
)
# Naively select the best 10, and compare everyone to the 10th best
best_10 <- top_n(all_players,10,mean)
best_10 <- best_10[order(best_10$mean,decreasing = TRUE),]
all_players <- all_players %>% mutate(
chance_better_than_10 = pnorm((mean - best_10$mean[10])/sqrt(sd^2 + best_10$sd[10]^2))
)
# In order to have a 5% chance of being better than the tenth best
# player in 10 of the 20 games, a player needs to have:
# 20 C 10 * x^10 = 0.05
# x = 0.22
# a 22% chance of being better than the tenth best player on a
# single game.
# All the remaining players:
ggplot(all_players, aes(x = mean, y = sd)) +
geom_point() +
ggtitle("Players for general season")  +
scale_x_continuous(limits = c(0,100))
# Same, but only the players with a >5% chance of being better
# than the 10th best player on average in over half the games
ggplot(filter(all_players, chance_better_than_10 > 0.22), aes(x = mean, y = sd)) +
geom_point() +
ggtitle("Top cut of players for general season") +
scale_x_continuous(limits = c(0,100))
a <- numeric(length(bucketed_players))
bucketed_players <- partition(all_players,"sd","mean",2.5,10)
# Test how many runs it'll take
a <- numeric(length(bucketed_players))
a[1] <- 10
i <- 0
while(sum(a)==10){
a <- increment_amounts(bucketed_players,a)
i <- i + 1
}
print(paste(i, "runs incoming"))
best_on_average_teams <- all_possible_teams
# 13.5 hour run successfully done and saved here, please don't overwrite
# write.csv(all_possible_teams, "../PROCESSED_DATA/all_possible_teams.csv")
all_possible_teams <- read_csv("../PROCESSED_DATA/all_possible_teams.csv")
best_on_average_teams <- all_possible_teams
best_on_average_teams <- best_on_average_teams[order(best_on_average_teams$avg_wins,
decreasing = TRUE),]
best_undefeated_teams <- top_n(all_possible_teams,30,undefeated_chance)
best_undefeated_teams <- best_undefeated_teams[order(best_undefeated_teams$undefeated_chance,
decreasing = TRUE),]
# You can just let this run for however long you have, it goes back
# through the existing data and retabulates the the success measures
# with maximum precision, in descending order- from the ones that
# performed best on the imprecise run to the ones that performed
# worst
for(i in 1:nrow(best_on_average_teams)) {
players <- filter(all_players,`student label` %in% best_on_average_teams[i,1:10])
team_pdf <- score_pdf_from_players(players$mean,players$sd)
team_cdf <- score_cdf_from_players(players$mean,players$sd)
results <- success_measures(win_chances$chance_winning, win_chances$score_difs,
team_pdf, team_cdf, match_ups, 2,
mean(players$mean) - 8 * sqrt(sum(players$sd^2)),
mean(players$mean) + 8 * sqrt(sum(players$sd^2)))
best_on_average_teams[i,11] <- results[1]
best_on_average_teams[i,12] <- results[2]
}
# You can just let this run for however long you have, it goes back
# through the existing data and retabulates the the success measures
# with maximum precision, in descending order- from the ones that
# performed best on the imprecise run to the ones that performed
# worst
for(i in 1:nrow(best_on_average_teams)) {
players <- filter(all_players,`student label` %in% best_on_average_teams[i,1:10])
team_pdf <- score_pdf_from_players(players$mean,players$sd)
team_cdf <- score_cdf_from_players(players$mean,players$sd)
results <- success_measures(win_chances$chance_winning, win_chances$score_difs,
team_pdf, team_cdf, match_ups, 2,
mean(players$mean) - 8 * sqrt(sum(players$sd^2)),
mean(players$mean) + 8 * sqrt(sum(players$sd^2)))
best_on_average_teams[i,11] <- results[1]
best_on_average_teams[i,12] <- results[2]
if(i %% 10 == 0) {
print(paste(i * 10,"out of",nrow(best_on_average_teams),"rows"))
}
}
View(all_possible_teams)
best_teams <- all_possible_teams %>% mutate(
priority = max(ntile(avg_wins,nrow(all_possible_teams)),
ntile(undefeated_chance,nrow(all_possible_teams)))
)
View(bc)
View(best_teams)
best_teams <- all_possible_teams %>% mutate(
priority = max(ntile(avg_wins,nrow(all_possible_teams)),
ntile(undefeated_chance,nrow(all_possible_teams)))
)
best_teams <- all_possible_teams %>% mutate(
a = ntile(avg_wins,nrow(all_possible_teams)),
b = ntile(undefeated_chance,nrow(all_possible_teams)),
priority = max(ntile(avg_wins,nrow(all_possible_teams)),
ntile(undefeated_chance,nrow(all_possible_teams)))
)
best_teams <- best_teams %>% mutate(
priority = min(avg_place,und_place)
)
best_teams <- all_possible_teams %>% mutate(
avg_place = ntile(avg_wins,nrow(all_possible_teams)),
und_place = ntile(undefeated_chance,nrow(all_possible_teams))
)
best_teams <- best_teams %>% mutate(
priority = min(avg_place,und_place)
)
best_teams[1,15]
best_teams[1,12]
best_teams[1,13]
best_teams <- all_possible_teams %>% mutate(
avg_place = ntile(avg_wins,nrow(all_possible_teams)),
und_place = ntile(undefeated_chance,nrow(all_possible_teams))
)
best_teams <- best_teams %>% mutate(
priority = min(avg_place,und_place)
)
best_teams <- best_teams[order(best_teams$priority,
decreasing = FALSE),]
best_undefeated_teams <- top_n(all_possible_teams,30,undefeated_chance)
best_undefeated_teams <- best_undefeated_teams[order(best_undefeated_teams$undefeated_chance,
decreasing = TRUE),]
# You can just let this run for however long you have, it goes back
# through the existing data and retabulates the the success measures
# with maximum precision, in descending order- from the ones that
# performed best on the imprecise run to the ones that performed
# worst
for(i in 1:nrow(best_on_average_teams)) {
players <- filter(all_players,`student label` %in% best_on_average_teams[i,1:10])
team_pdf <- score_pdf_from_players(players$mean,players$sd)
team_cdf <- score_cdf_from_players(players$mean,players$sd)
results <- success_measures(win_chances$chance_winning, win_chances$score_difs,
team_pdf, team_cdf, match_ups, 2,
mean(players$mean) - 8 * sqrt(sum(players$sd^2)),
mean(players$mean) + 8 * sqrt(sum(players$sd^2)))
best_on_average_teams[i,12] <- results[1]
best_on_average_teams[i,13] <- results[2]
if(best_on_average_teams[i,16] %% 10 == 0) {
print(paste(best_on_average_teams[i,16],"out of",nrow(best_on_average_teams),"rows"))
}
}
best_teams <- best_teams %>% mutate(
priority = min(c(avg_place,und_place))
)
best_teams <- all_possible_teams %>% mutate(
avg_place = ntile(avg_wins,nrow(all_possible_teams)),
und_place = ntile(undefeated_chance,nrow(all_possible_teams))
)
best_teams <- best_teams %>% mutate(
priority = min(c(avg_place,und_place))
)
best_teams <- all_possible_teams %>% mutate(
avg_place = ntile(avg_wins,nrow(all_possible_teams)),
und_place = ntile(undefeated_chance,nrow(all_possible_teams))
)
View(all_possible_teams)
best_teams <- all_possible_teams %>% mutate(
avg_place = ntile(avg_wins,nrow(all_possible_teams)),
und_place = ntile(undefeated_chance,nrow(all_possible_teams))
)
best_teams <- best_teams %>% mutate(
priority = min(c(avg_place,und_place))
)
best_teams <- all_possible_teams %>% mutate(
avg_place = ntile(avg_wins,nrow(all_possible_teams)),
und_place = ntile(undefeated_chance,nrow(all_possible_teams)),
priority = nrow(all_possible_teams)
)
best_teams[1,"und_place"]
for(i in 1:nrow(best_teams)) {
best_teams[i,"priority"] = min(best_teams[i,"und_place"],
best_teams[i,"avg_place"])
}
best_teams <- best_teams[order(best_teams$priority,
decreasing = FALSE),]
# You can just let this run for however long you have, it goes back
# through the existing data and retabulates the the success measures
# with maximum precision, in descending order- from the ones that
# performed best on the imprecise run to the ones that performed
# worst
for(i in 1:nrow(best_on_average_teams)) {
players <- filter(all_players,`student label` %in% best_on_average_teams[i,1:10])
team_pdf <- score_pdf_from_players(players$mean,players$sd)
team_cdf <- score_cdf_from_players(players$mean,players$sd)
results <- success_measures(win_chances$chance_winning, win_chances$score_difs,
team_pdf, team_cdf, match_ups, 2,
mean(players$mean) - 8 * sqrt(sum(players$sd^2)),
mean(players$mean) + 8 * sqrt(sum(players$sd^2)))
best_on_average_teams[i,12] <- results[1]
best_on_average_teams[i,13] <- results[2]
if(best_on_average_teams[i,16] %% 10 == 0) {
print(paste(best_on_average_teams[i,16],"out of",nrow(best_on_average_teams),"rows"))
}
}
best_teams <- all_possible_teams %>% mutate(
avg_place = ntile(avg_wins,nrow(all_possible_teams)),
und_place = ntile(undefeated_chance,nrow(all_possible_teams)),
priority = nrow(all_possible_teams)
)
for(i in 1:nrow(best_teams)) {
best_teams[i,"priority"] = min(best_teams[i,"und_place"],
best_teams[i,"avg_place"])
}
best_teams <- best_teams[order(best_teams$priority,
decreasing = FALSE),]
# You can just let this run for however long you have, it goes back
# through the existing data and retabulates the the success measures
# with maximum precision, in descending order- from the ones that
# performed best on the imprecise run to the ones that performed
# worst
for(i in 1:nrow(best_on_average_teams)) {
players <- filter(all_players,`student label` %in% best_on_average_teams[i,1:10])
team_pdf <- score_pdf_from_players(players$mean,players$sd)
team_cdf <- score_cdf_from_players(players$mean,players$sd)
results <- success_measures(win_chances$chance_winning, win_chances$score_difs,
team_pdf, team_cdf, match_ups, 2,
mean(players$mean) - 8 * sqrt(sum(players$sd^2)),
mean(players$mean) + 8 * sqrt(sum(players$sd^2)))
best_on_average_teams[i,"avg_wins"] <- results[1]
best_on_average_teams[i,"undefeated_chance"] <- results[2]
if(best_on_average_teams[i,"priority"] %% 10 == 0) {
print(paste(best_on_average_teams[i,"priority"],"out of",nrow(best_on_average_teams),"rows"))
}
}
# You can just let this run for however long you have, it goes back
# through the existing data and retabulates the the success measures
# with maximum precision, in descending order- from the ones that
# performed best on the imprecise run to the ones that performed
# worst
for(i in 1:nrow(best_teams)) {
players <- filter(all_players,`student label` %in% best_teams[i,1:10])
team_pdf <- score_pdf_from_players(players$mean,players$sd)
team_cdf <- score_cdf_from_players(players$mean,players$sd)
results <- success_measures(win_chances$chance_winning, win_chances$score_difs,
team_pdf, team_cdf, match_ups, 2,
mean(players$mean) - 8 * sqrt(sum(players$sd^2)),
mean(players$mean) + 8 * sqrt(sum(players$sd^2)))
best_teams[i,"avg_wins"] <- results[1]
best_teams[i,"undefeated_chance"] <- results[2]
if(best_teams[i,"priority"] %% 10 == 0) {
print(paste(best_teams[i,"priority"],"out of",nrow(best_on_average_teams),"rows"))
}
}
# Notably, the #1 spot doesn't change on this one
best_on_average_teams <- best_teams[order(best_teams$avg_wins,
decreasing = TRUE),]
best_on_average_teams <- best_on_average_teams[1:10,]
best_undefeated_teams <- best_teams[order(best_teams$undefeated_chance,
decreasing = TRUE),]
best_undefeated_teams <- best_undefeated_teams[1:10,]
View(best_on_average_teams)
View(best_undefeated_teams)
ggplot(previous_results, aes(x=score_dif)) +
geom_histogram(binwidth=5, color="grey50", fill="lightblue", alpha=0.8) +
theme_minimal() +
stat_function(fun = function(x){dnorm(x,mean = mean(previous_results),sd = sd(previous_results))}*length(previous_results)*5)
ggplot(previous_results, aes(x=score_dif)) +
geom_histogram(binwidth=5, color="grey50", fill="lightblue", alpha=0.8) +
theme_minimal() +
stat_function(fun = function(x){dnorm(x,mean = mean(previous_results$score_dif)
,sd = sd(previous_results$score_dif))}*length(previous_results$score_dif)*5)
View(eveningmodel)
knitr::stitch('analysis.r')
library(tinytex)
knitr::stitch('analysis.r')
knitr::stitch_rhtml('analysis.r')
knitr::stitch_rhtml('helper_functions.r')
knitr::stitch_rhtml('previous_results_probabilities.r')
knitr::stitch_rhtml('previous results probabilities.r')
knitr::stitch_rhtml('data_processing.r')
knitr::stitch_rhtml('./CSV_PROCESSING/data_processing.r')
knitr::stitch_rhtml('../CSV_PROCESSING/data_processing.r')
